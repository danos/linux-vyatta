From: Peter Zijlstra <peterz@infradead.org>
Date: Tue, 16 Mar 2010 14:31:44 -0700
Subject: [PATCH 147/397] sched: Break out from load_balancing on rq_lock
 contention
Origin: https://git.kernel.org/cgit/linux/kernel/git/rt/linux-stable-rt.git/commit?id=0d845e13eacf66ddaafc0f1bf0a8ff40731b1f1c

Also limit NEW_IDLE pull

Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
---
 kernel/sched_fair.c | 18 ++++++++++++++++++
 1 file changed, 18 insertions(+)

diff --git a/kernel/sched_fair.c b/kernel/sched_fair.c
index 98e103988aad..1c18fe84eabc 100644
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -2910,6 +2910,10 @@ balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,
 		 */
 		if (idle == CPU_NEWLY_IDLE)
 			break;
+
+		if (raw_spin_is_contended(&this_rq->lock) ||
+		    raw_spin_is_contended(&busiest->lock))
+			break;
 #endif
 
 		/*
@@ -3050,6 +3054,20 @@ load_balance_fair(struct rq *this_rq, int this_cpu, struct rq *busiest,
 		rem_load_move -= moved_load;
 		if (rem_load_move < 0)
 			break;
+
+#ifdef CONFIG_PREEMPT
+		/*
+		 * NEWIDLE balancing is a source of latency, so preemptible
+		 * kernels will stop after the first task is pulled to minimize
+		 * the critical section.
+		 */
+		if (idle == CPU_NEWLY_IDLE && this_rq->nr_running)
+			break;
+
+		if (raw_spin_is_contended(&this_rq->lock) ||
+		    raw_spin_is_contended(&busiest->lock))
+			break;
+#endif
 	}
 	rcu_read_unlock();
 
